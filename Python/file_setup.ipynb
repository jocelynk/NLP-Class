{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from itertools import izip\n",
    "from sklearn.cross_validation import KFold\n",
    "import HTMLParser\n",
    "import argparse\n",
    "\n",
    "TRAIN_FILE = \"../data/train.txt\"\n",
    "TEST_FILE = \"../data/DevNoLabels.txt\"\n",
    "#TEST_FILE = \"../tweebo/inputs/test.nolabels.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Setup Twitter input files for TweeboParser\n",
    "\n",
    "#formated inputs for Tweebo\n",
    "def process_files_for_tweebo(train_file, test_file):\n",
    "    tweebo_input_train_file = \"./data/train_tweebo_input.txt\"\n",
    "    tweebo_input_test_file = \"./data/test_tweebo_input.txt\"\n",
    "    with open(train_file) as train:\n",
    "        input_lines = train.readlines()\n",
    "        tweets = []\n",
    "        arr = []\n",
    "        for line in input_lines:\n",
    "            try:\n",
    "                if line in ['\\n', '\\r\\n']:\n",
    "                    sentence = ' '.join(str(x) for x in arr) \n",
    "                    tweets.append(sentence.strip())\n",
    "                    arr = []\n",
    "                else:\n",
    "                    (word, tag) = line.split()\n",
    "                    arr.append(word)\n",
    "            except ValueError:\n",
    "                continue\n",
    "        #open(tweebo_input_train_file, 'w').close()\n",
    "        with open (tweebo_input_train_file, 'w') as f: \n",
    "            for tweet in tweets:\n",
    "                f.write(tweet)\n",
    "                f.write(\"\\n\")\n",
    "\n",
    "    with open(test_file) as test:\n",
    "        input_lines = test.readlines()\n",
    "        tweets = []\n",
    "        arr = []\n",
    "        for line in input_lines:\n",
    "            try:\n",
    "                if line in ['\\n', '\\r\\n']:\n",
    "                    sentence = ' '.join(str(x) for x in arr) \n",
    "                    tweets.append(sentence.strip())\n",
    "                    arr = []\n",
    "                else:\n",
    "                    word = line.strip()\n",
    "                    arr.append(word)\n",
    "            except ValueError:\n",
    "                continue\n",
    "\n",
    "        #open(tweebo_input_test_file, 'w').close()\n",
    "        with open (tweebo_input_test_file, 'w') as f: \n",
    "            for tweet in tweets:\n",
    "                f.write(tweet)\n",
    "                f.write(\"\\n\")            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Setup Train/Test Tweebo files to extract features\n",
    "\n",
    "# train_tweebo_file = \"C:/Users/User/Documents/Cornell/Courses/NLP/HW4/tweebo/inputs/train_input.txt.predict\"\n",
    "# test_tweebo_file = \"C:/Users/User/Documents/Cornell/Courses/NLP/HW4/tweebo/inputs/tweebo_test.nolabels.txt.predict\"\n",
    "# file_to_train = test_file\n",
    "# file_to_train_tweebo = test_tweebo_file\n",
    "# result_file = \"C:/Users/User/Documents/Cornell/Courses/NLP/HW4/tweebo/inputs/test.nolabels.processed.txt\"\n",
    "\n",
    "def process_tweebo_files(kaggle, file_to_train, file_to_train_tweebo, result_file):\n",
    "    with open(file_to_train) as train:\n",
    "        input_lines = train.readlines()\n",
    "        labeled_tweets = []\n",
    "        arr = []\n",
    "        for line in input_lines:\n",
    "            try:\n",
    "                if line in ['\\n', '\\r\\n']:\n",
    "                    labeled_tweets.append(arr)\n",
    "                    arr = []\n",
    "                else:\n",
    "                    if not kaggle:\n",
    "                        (word, tag) = line.split()\n",
    "                        arr.append((word, tag))\n",
    "                    else:\n",
    "                        arr.append([line.strip()])\n",
    "\n",
    "            except ValueError:\n",
    "                continue\n",
    "\n",
    "    with open(file_to_train_tweebo) as train_tweebo:\n",
    "        input_lines = train_tweebo.readlines()\n",
    "        tweebo_tweets = []\n",
    "        arr = []\n",
    "        for line in input_lines:\n",
    "            try:\n",
    "                if line in ['\\n', '\\r\\n']:\n",
    "                    tweebo_tweets.append(arr)\n",
    "                    arr = []\n",
    "                else:\n",
    "                    (word_id, word, lemma, cpostag, postag, feats, head, dep_relation) = line.split()\n",
    "                    arr.append((word, word_id, postag, head, dep_relation));\n",
    "            except ValueError:\n",
    "                continue\n",
    "\n",
    "    features = []\n",
    "\n",
    "    for s1, s2 in zip(labeled_tweets, tweebo_tweets):\n",
    "        tweebo_index = 0\n",
    "        sentence = []\n",
    "        head_diff = 0\n",
    "        head_point = 0\n",
    "        head_point_arr = []\n",
    "        head_diff_arr = []\n",
    "        for i in range(len(s1)):\n",
    "            word1 = s1[i][0]\n",
    "            escaped_word1 = HTMLParser.HTMLParser().unescape(s1[i][0]) \n",
    "            word2 = HTMLParser.HTMLParser().unescape(s2[tweebo_index][0]) \n",
    "            t_ind = tweebo_index\n",
    "            if(word1 != word2 and escaped_word1 != word2):\n",
    "                head_point = i+1\n",
    "                head_point_arr.append(head_point)\n",
    "                while True:\n",
    "                    if(word2 == word1 or word2 == escaped_word1):\n",
    "                        if((int(s2[tweebo_index][3]) != -1 and t_ind != tweebo_index)):\n",
    "                            t_ind = tweebo_index\n",
    "                        break;\n",
    "                    else:\n",
    "                        head_diff += 1\n",
    "                        if((int(s2[tweebo_index][3]) != -1 and t_ind != tweebo_index)):\n",
    "                            t_ind = tweebo_index\n",
    "                        tweebo_index += 1\n",
    "                        word2 += HTMLParser.HTMLParser().unescape(s2[tweebo_index][0])\n",
    "                head_diff_arr.append(head_diff)\n",
    "\n",
    "            if not kaggle:\n",
    "                sentence.append([i+1, escaped_word1, s1[i][1], s2[t_ind][2], int(s2[t_ind][3]), s2[t_ind][4]])\n",
    "            else:\n",
    "                sentence.append([i+1, escaped_word1, s2[t_ind][2], int(s2[t_ind][3]), s2[t_ind][4]])\n",
    "            tweebo_index += 1\n",
    "        #head indexes reconciliation\n",
    "        #print head_point_arr\n",
    "        if len(head_diff_arr) > 0:\n",
    "            #print sentence\n",
    "            for word in sentence:\n",
    "                if not kaggle:\n",
    "                    old_head = word[4]\n",
    "                else:\n",
    "                    old_head = word[3]\n",
    "                if(old_head == -1 or old_head == 0):\n",
    "                    continue\n",
    "\n",
    "                start = old_head - (len(s2) - len(s1)) - 2\n",
    "                end = old_head + (len(s2) - len(s1))\n",
    "\n",
    "                if(start < 0):\n",
    "                    start = 0\n",
    "                if(end > len(sentence)):\n",
    "                    end = len(sentence)\n",
    "\n",
    "                old_word = s2[old_head-1][0]\n",
    "\n",
    "                if(old_head < len(s2)):\n",
    "                    next_word = s2[old_head][0]\n",
    "                else:\n",
    "                    next_word = ''\n",
    "\n",
    "                old_word_next = old_word + next_word\n",
    "\n",
    "                if(old_head > 0):\n",
    "                    prev_word = s2[old_head-2][0]\n",
    "                else:\n",
    "                    prev_word = ''\n",
    "\n",
    "                prev_old_word = prev_word + old_word\n",
    "\n",
    "                new_word = ''\n",
    "\n",
    "                for i in xrange(start, end):\n",
    "                    if(HTMLParser.HTMLParser().unescape(old_word) == sentence[i][1] \\\n",
    "                       or HTMLParser.HTMLParser().unescape(old_word_next) in sentence[i][1]\\\n",
    "                      or HTMLParser.HTMLParser().unescape(prev_old_word) in sentence[i][1]):                    \n",
    "                        new_word = sentence[i]\n",
    "                        break\n",
    "                if not kaggle:\n",
    "                    word[4] = new_word[0]\n",
    "                else:\n",
    "                    word[3] = new_word[0]\n",
    "\n",
    "        features.append(sentence)\n",
    "\n",
    "    open(result_file, 'w').close()\n",
    "    with open (result_file, 'w') as f: \n",
    "        for tweet in features:\n",
    "            for word in tweet:\n",
    "                f.write(' '.join(str(x) for x in word).strip())\n",
    "                f.write(\"\\n\")\n",
    "            f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main(argv):    \n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('-train', help='Train File Path', dest='train', required=True)\n",
    "    parser.add_argument('-test', help='Test File Path', dest='test', required=True)\n",
    "    parser.add_argument('-train-tweebo', help='Tweebo-processed Train File Path', dest='train_tweebo')\n",
    "    parser.add_argument('-test-tweebo', help='Tweebo-processed Test File Path', dest='test_tweebo')\n",
    "    parser.add_argument('-prep-tweebo', help='1 = Prep Files to use for Tweebo Parser, 0 = Do not prep files', dest='prep_tweebo', default=0, type=int)\n",
    "    parser.add_argument('-parse-tweebo-train', help='1 = Parse Tweebo Train Files, 0 = Do not parse file', dest='parse_tweebo_train', default=0, type=int)\n",
    "    parser.add_argument('-parse-tweebo-test', help='1 = Parse Tweebo Test Files, 0 = Do not parse file', dest='parse_tweebo_test', default=0, type=int)\n",
    "    parser.add_argument('-kaggle-data', help='1 = Parsing test data for Kaggle, 0 = test data not for Kaggle', dest='kaggle', default=0, type=int)\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    if bool(args.prep_tweebo):\n",
    "        process_files_for_tweebo(args.train, args.test)\n",
    "    if bool(args.parse_tweebo_train):\n",
    "        result_file = './data/train-tweebo.processed.txt'\n",
    "        process_tweebo_files(args.kaggle, args.train, args.train_tweebo)\n",
    "    if bool(args.parse_tweebo_test):\n",
    "        result_file = './data/test-tweebo.processed.txt'\n",
    "        process_tweebo_files(args.kaggle, args.test, args.test_tweebo)\n",
    "        \n",
    "    \n",
    "             \n",
    "if __name__ == '__main__':\n",
    "    main(sys.argv[1:])\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
