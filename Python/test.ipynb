{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import HTMLParser\n",
    "no_labels = \"./test.nolabels.txt\"\n",
    "with_labels = \"./test_withlabels.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2377\n",
      "3336\n",
      "# not in withlabels file: 0\n"
     ]
    }
   ],
   "source": [
    "with open(no_labels) as t:\n",
    "    input_lines = t.readlines()\n",
    "    tweets = []\n",
    "    arr = []\n",
    "    for line in input_lines:\n",
    "        try:\n",
    "            if line in ['\\n', '\\r\\n']:\n",
    "                sentence = ' '.join(str(x) for x in arr) \n",
    "                tweets.append(sentence.strip())\n",
    "                arr = []\n",
    "            else:\n",
    "                word = line.strip()\n",
    "                arr.append(word)\n",
    "        except ValueError:\n",
    "            continue\n",
    "\n",
    "with open(with_labels) as t:\n",
    "    input_lines = t.readlines()\n",
    "    tweets_new = []\n",
    "    arr = []\n",
    "    arr2 = []\n",
    "    tags = []\n",
    "    for line in input_lines:\n",
    "        try:\n",
    "            if line in ['\\n', '\\r\\n']:\n",
    "                sentence = ' '.join(str(x) for x in arr) \n",
    "                tweets_new.append(sentence.strip())\n",
    "                tags.append(arr2)\n",
    "                arr = []\n",
    "                arr2 = []\n",
    "            else:\n",
    "                (word, tag) = line.split()\n",
    "                arr.append(word)\n",
    "                arr2.append(tag)\n",
    "        except ValueError:\n",
    "            continue\n",
    "\n",
    "print len(tweets)\n",
    "print len(tweets_new)\n",
    "# with open(extra) as e:\n",
    "#     input_lines = e.readlines()\n",
    "#     sentences = []\n",
    "#     arr_extra = []\n",
    "#     pairs = []\n",
    "#     arr_extra_pairs = []\n",
    "#     for line in input_lines:\n",
    "#         try:\n",
    "#             if line in ['\\n', '\\r\\n']:\n",
    "#                 sentence = ' '.join(str(x) for x in arr_extra)\n",
    "#                 sentences.append(sentence.strip())\n",
    "#                 pairs.append(arr_extra_pairs)\n",
    "#                 arr_extra = []\n",
    "#                 arr_extra_pairs = []\n",
    "#             else:\n",
    "#                 (word, tag) = line.split()\n",
    "#                 if('B' in tag):\n",
    "#                     tag = 'B'\n",
    "#                 elif('I' in tag):\n",
    "#                     tag = 'I'\n",
    "#                 arr_extra.append(word)\n",
    "#                 arr_extra_pairs.append([word, tag])\n",
    "#         except ValueError:\n",
    "#             continue\n",
    "\n",
    "count = 0\n",
    "new_test = []\n",
    "new_test_sentences = []\n",
    "i = 0\n",
    "for t in tweets:\n",
    "    if(t not in tweets_new):\n",
    "        count += 1\n",
    "        new_test_sentences.append(t)\n",
    "        #new_test.append(pairs[i])\n",
    "    i += 1\n",
    "\n",
    "\n",
    "print '# not in withlabels file: ' + str(count)\n",
    "\n",
    "open(\"./test_small_withlabels.txt\", 'w').close()\n",
    "with open ('./test_small_withlabels.txt', 'w') as f: \n",
    "    for t in tweets:\n",
    "        if(t in tweets_new):\n",
    "            index = tweets_new.index(t)\n",
    "            tweet_tags = tags[index]\n",
    "            \n",
    "            tweet = t.split()\n",
    "            for i in range(len(tweet)):\n",
    "                f.write(tweet[i] + \"\\t\" + tweet_tags[i] + \"\\n\")\n",
    "            f.write(\"\\n\")\n",
    "                \n",
    "        \n",
    "\n",
    "# open(tweebo_file, 'w').close()\n",
    "# with open (tweebo_file, 'w') as f: \n",
    "#     for s in new_test_sentences:\n",
    "#         f.write(s)\n",
    "#         f.write(\"\\n\") \n",
    "            \n",
    "# open(extra_labeled, 'w').close()\n",
    "# with open (extra_labeled, 'w') as f: \n",
    "#     for i in new_test:\n",
    "#         for j in i:\n",
    "#             f.write(j[0] + \" \" + j[1] + \"\\n\")\n",
    "#         f.write(\"\\n\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kill or be killed . This is life .\n",
      "Kill or be killed . This is life .\n"
     ]
    }
   ],
   "source": [
    "print tweets[0]\n",
    "print tweets_new[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Setup Train/Test Tweebo files to extract features\n",
    "\n",
    "file_to_train = extra_labeled\n",
    "file_to_train_tweebo = tweebo_features\n",
    "result_file = \"C:/Users/User/Documents/Cornell/Courses/NLP/HW4/Python/extra_tweebo_features.txt\"\n",
    "kaggle = False\n",
    "with open(file_to_train) as train:\n",
    "    input_lines = train.readlines()\n",
    "    labeled_tweets = []\n",
    "    arr = []\n",
    "    for line in input_lines:\n",
    "        try:\n",
    "            if line in ['\\n', '\\r\\n']:\n",
    "                labeled_tweets.append(arr)\n",
    "                arr = []\n",
    "            else:\n",
    "                if not kaggle:\n",
    "                    (word, tag) = line.split()\n",
    "                    arr.append((word, tag))\n",
    "                else:\n",
    "                    arr.append([line.strip()])\n",
    "                \n",
    "        except ValueError:\n",
    "            continue\n",
    "\n",
    "with open(file_to_train_tweebo) as train_tweebo:\n",
    "    input_lines = train_tweebo.readlines()\n",
    "    tweebo_tweets = []\n",
    "    arr = []\n",
    "    for line in input_lines:\n",
    "        try:\n",
    "            if line in ['\\n', '\\r\\n']:\n",
    "                tweebo_tweets.append(arr)\n",
    "                arr = []\n",
    "            else:\n",
    "                (word_id, word, lemma, cpostag, postag, feats, head, dep_relation) = line.split()\n",
    "                arr.append((word, word_id, postag, head, dep_relation));\n",
    "        except ValueError:\n",
    "            continue\n",
    "            \n",
    "features = []\n",
    "\n",
    "for s1, s2 in zip(labeled_tweets, tweebo_tweets):\n",
    "    tweebo_index = 0\n",
    "    sentence = []\n",
    "    head_diff = 0\n",
    "    head_point = 0\n",
    "    head_point_arr = []\n",
    "    head_diff_arr = []\n",
    "    for i in range(len(s1)):\n",
    "        word1 = s1[i][0]\n",
    "        escaped_word1 = HTMLParser.HTMLParser().unescape(s1[i][0]) \n",
    "        word2 = HTMLParser.HTMLParser().unescape(s2[tweebo_index][0]) \n",
    "        t_ind = tweebo_index\n",
    "        if(word1 != word2 and escaped_word1 != word2):\n",
    "            head_point = i+1\n",
    "            head_point_arr.append(head_point)\n",
    "            while True:\n",
    "                if(word2 == word1 or word2 == escaped_word1):\n",
    "                    break;\n",
    "                else:\n",
    "                    head_diff += 1\n",
    "                    if(int(s2[tweebo_index][3]) == -1 and int(s2[t_ind][3]) == -1):\n",
    "                        t_ind += 1\n",
    "                    tweebo_index += 1\n",
    "                    word2 += HTMLParser.HTMLParser().unescape(s2[tweebo_index][0])\n",
    "            head_diff_arr.append(head_diff)\n",
    "            \n",
    "        if not kaggle:\n",
    "            sentence.append([i+1, escaped_word1, s1[i][1], s2[t_ind][2], int(s2[t_ind][3]), s2[t_ind][4]])\n",
    "        else:\n",
    "            sentence.append([i+1, escaped_word1, s2[t_ind][2], int(s2[t_ind][3]), s2[t_ind][4]])\n",
    "        tweebo_index += 1\n",
    "    #head indexes reconciliation\n",
    "    for word in sentence:\n",
    "        if not kaggle:\n",
    "            old_head = word[4]\n",
    "        else:\n",
    "            old_head = word[3]\n",
    "            \n",
    "        for h in range(len(head_point_arr)):\n",
    "            if(old_head < head_point_arr[h]):\n",
    "                break;\n",
    "            elif(old_head > head_point_arr[h] and h == (len(head_point_arr) - 1)):\n",
    "                if not kaggle:\n",
    "                    word[4] = old_head - head_diff_arr[h]\n",
    "                else:\n",
    "                    word[3] = old_head - head_diff_arr[h]\n",
    "                \n",
    "            elif(old_head > head_point_arr[h] and old_head < head_point_arr[h+1]):\n",
    "                if not kaggle:\n",
    "                    word[4] = old_head - head_diff_arr[h]\n",
    "                else:\n",
    "                    word[3] = old_head - head_diff_arr[h]\n",
    "            else:\n",
    "                continue\n",
    "    \n",
    "    features.append(sentence)\n",
    "    \n",
    "open(result_file, 'w').close()\n",
    "with open (result_file, 'w') as f: \n",
    "    for tweet in features:\n",
    "        for word in tweet:\n",
    "            f.write(' '.join(str(x) for x in word).strip())\n",
    "            f.write(\"\\n\")\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "with open('C:\\Users\\User\\Documents\\Cornell\\Courses\\NLP\\HW4\\data\\kaggle_crf_result.txt') as t:\n",
    "    input_lines = t.readlines()\n",
    "    arr = []\n",
    "    for line in input_lines:\n",
    "        arr.append(line)\n",
    "            \n",
    "with open('C:\\Users\\User\\Documents\\Cornell\\Courses\\NLP\\HW4\\data\\kaggle_crf_result2.txt') as t2:\n",
    "    input_lines = t2.readlines()\n",
    "    arr2 = []\n",
    "    for line in input_lines:\n",
    "        arr2.append(line)\n",
    "\n",
    "count = 0\n",
    "for i in range(len(arr2)):\n",
    "    if(arr2[i] != arr[i]):\n",
    "        count += 1\n",
    "        \n",
    "print count"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
